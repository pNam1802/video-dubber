import os
import sys
import shutil
import whisper
import torch
import subprocess
from transformers import MarianMTModel, MarianTokenizer
import asyncio
import edge_tts
from config import Config
import re
from openai import OpenAI

# Set FFmpeg path - try multiple locations
def get_ffmpeg_path():
    """T√¨m ffmpeg trong system PATH ho·∫∑c c√°c v·ªã tr√≠ th∆∞·ªùng g·∫∑p"""
    # C√°ch 1: T√¨m trong PATH (n·∫øu ffmpeg ƒë∆∞·ª£c c√†i ƒë·∫∑t to√†n c·ª•c)
    try:
        result = subprocess.run(['where', 'ffmpeg'], capture_output=True, text=True)
        if result.returncode == 0:
            return result.stdout.strip().split('\n')[0]
    except:
        pass
    
    # C√°ch 2: C√°c v·ªã tr√≠ th∆∞·ªùng g·∫∑p tr√™n Windows
    common_paths = [
        r'C:\ffmpeg-master-latest-win64-gpl\ffmpeg-master-latest-win64-gpl\bin\ffmpeg.exe',
        r'C:\ffmpeg\bin\ffmpeg.exe',
        r'C:\Program Files\ffmpeg\bin\ffmpeg.exe',
        r'C:\Program Files (x86)\ffmpeg\bin\ffmpeg.exe',
    ]
    
    for path in common_paths:
        if os.path.exists(path):
            return path
    
    # C√°ch 3: ƒê∆°n gi·∫£n l√† "ffmpeg" (s·∫Ω search trong PATH)
    return 'ffmpeg'

ffmpeg_path = get_ffmpeg_path()
print(f'[INFO] Using FFmpeg: {ffmpeg_path}')

# Th√™m th∆∞ m·ª•c ch·ª©a ffmpeg v√†o PATH
ffmpeg_dir = os.path.dirname(ffmpeg_path)
if ffmpeg_dir and os.path.exists(ffmpeg_dir):
    os.environ['PATH'] = ffmpeg_dir + ';' + os.environ.get('PATH', '')

# Dictionary c√°c t·ª´ kh√≥a chuy√™n ng√†nh ML c·∫ßn gi·ªØ nguy√™n
ML_KEYWORDS = {
    # Neural Networks
    'neural network': '__ML_TERM_0__',
    'deep learning': '__ML_TERM_1__',
    'convolutional': '__ML_TERM_2__',
    'recurrent': '__ML_TERM_3__',
    'lstm': '__ML_TERM_4__',
    'transformer': '__ML_TERM_5__',
    'attention': '__ML_TERM_6__',
    'backpropagation': '__ML_TERM_7__',
    
    # Training & Optimization
    'gradient descent': '__ML_TERM_8__',
    'optimization': '__ML_TERM_9__',
    'loss function': '__ML_TERM_10__',
    'activation function': '__ML_TERM_11__',
    'batch normalization': '__ML_TERM_12__',
    'dropout': '__ML_TERM_13__',
    'regularization': '__ML_TERM_14__',
    'overfitting': '__ML_TERM_15__',
    'underfitting': '__ML_TERM_16__',
    
    # Data & Features
    'feature extraction': '__ML_TERM_17__',
    'feature engineering': '__ML_TERM_18__',
    'dataset': '__ML_TERM_19__',
    'training set': '__ML_TERM_20__',
    'validation set': '__ML_TERM_21__',
    'test set': '__ML_TERM_22__',
    'preprocessing': '__ML_TERM_23__',
    'normalization': '__ML_TERM_24__',
    'augmentation': '__ML_TERM_25__',
    
    # Algorithms
    'classification': '__ML_TERM_26__',
    'regression': '__ML_TERM_27__',
    'clustering': '__ML_TERM_28__',
    'supervised': '__ML_TERM_29__',
    'unsupervised': '__ML_TERM_30__',
    'reinforcement learning': '__ML_TERM_31__',
    'random forest': '__ML_TERM_32__',
    'support vector machine': '__ML_TERM_33__',
    'k-means': '__ML_TERM_34__',
    
    # Evaluation
    'accuracy': '__ML_TERM_35__',
    'precision': '__ML_TERM_36__',
    'recall': '__ML_TERM_37__',
    'f1-score': '__ML_TERM_38__',
    'confusion matrix': '__ML_TERM_39__',
    'roc curve': '__ML_TERM_40__',
    'auc': '__ML_TERM_41__',
    
    # Frameworks & Tools
    'tensorflow': '__ML_TERM_42__',
    'pytorch': '__ML_TERM_43__',
    'scikit-learn': '__ML_TERM_44__',
    'keras': '__ML_TERM_45__',
    'numpy': '__ML_TERM_46__',
    'pandas': '__ML_TERM_47__',
}

# Global task status store
task_status = {}

# Model caching
_whisper_model = None
_translation_model = None
_tokenizer = None
_device = None

def get_device():
    global _device
    if _device is None:
        _device = "cuda" if torch.cuda.is_available() else "cpu"
    return _device

def get_whisper_model():
    global _whisper_model
    if _whisper_model is None:
        _whisper_model = whisper.load_model("small")  # Upgraded from "base"
    return _whisper_model

def get_translation_model():
    global _translation_model, _tokenizer
    if _translation_model is None:
        model_name = "Helsinki-NLP/opus-mt-en-vi"
        _tokenizer = MarianTokenizer.from_pretrained(model_name)
        _translation_model = MarianMTModel.from_pretrained(model_name).to(get_device())
    return _translation_model, _tokenizer

async def generate_voice(text, output_path):
    communicate = edge_tts.Communicate(text, "vi-VN-HoaiMyNeural") # Gi·ªçng n·ªØ VN
    await communicate.save(output_path)

def get_audio_duration(audio_path):
    """L·∫•y th·ªùi l∆∞·ª£ng audio b·∫±ng ffprobe"""
    try:
        cmd = [
            'ffprobe',
            '-v', 'error',
            '-show_entries', 'format=duration',
            '-of', 'default=noprint_wrappers=1:nokey=1:noprint_wrappers=1',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', timeout=10)
        if result.returncode == 0:
            return float(result.stdout.strip())
    except Exception as e:
        print(f'[WARNING] Cannot get audio duration: {e}')
    return None

def find_natural_breaks(text):
    """
    T√¨m v·ªã tr√≠ t·ª± nhi√™n ƒë·ªÉ ch√®n pause: d·∫•u c√¢u, t·ª´ n·ªëi, v.v.
    Tr·∫£ v·ªÅ danh s√°ch v·ªã tr√≠ (0-1) d·ª±a tr√™n k√Ω t·ª±
    """
    breaks = []
    # T√¨m d·∫•u ch·∫•m, ph·∫©y, d·∫•u h·ªèi, d·∫•u ch√©m
    punctuation_positions = []
    for i, char in enumerate(text):
        if char in '.,!?;:‚Äî‚Äì':
            punctuation_positions.append(i / len(text))
    
    # T√¨m t·ª´ n·ªëi (connectors): "and", "but", "however", "therefore"
    connectors = ['and', 'but', 'however', 'therefore', 'also', 'because', 'since', 'although']
    connector_positions = []
    text_lower = text.lower()
    for conn in connectors:
        import re
        for match in re.finditer(r'\b' + conn + r'\b', text_lower):
            connector_positions.append(match.start() / len(text))
    
    # K·∫øt h·ª£p v√† s·∫Øp x·∫øp
    breaks = sorted(set(punctuation_positions + connector_positions))
    return breaks if breaks else [0.5]  # Default: gi·ªØa text

def add_natural_pauses(audio_path, text, target_duration, output_path):
    """
    Th√™m pause t·ª± nhi√™n v√†o v·ªã tr√≠ ph√π h·ª£p
    """
    try:
        current_duration = get_audio_duration(audio_path)
        if not current_duration:
            return False
        
        # N·∫øu ƒë√£ ƒë·ªß th·ªùi gian, return
        if current_duration >= target_duration * 0.95:
            return True
        
        pause_positions = find_natural_breaks(text)
        total_pause_needed = target_duration - current_duration
        pause_per_break = total_pause_needed / len(pause_positions)
        
        # Gi·ªõi h·∫°n pause: 50ms - 300ms t√πy v·ªã tr√≠
        pause_duration = min(0.3, max(0.05, pause_per_break))
        
        # T·∫°o filter complex ƒë·ªÉ ch√®n silence
        delays = []
        current_offset = 0
        for i, pos in enumerate(pause_positions):
            delays.append(f"[0:a]adelay={int(current_offset*1000)}|{int(current_offset*1000)}[seg{i}]")
            current_offset += pause_duration
        
        # Build FFmpeg filter
        segments = ''.join([f"[seg{i}]" for i in range(len(pause_positions))])
        filter_complex = ';'.join(delays) + f";{segments}concat=n={len(pause_positions)}:v=0:a=1[aout]"
        
        cmd = [
            ffmpeg_path,
            '-i', audio_path,
            '-filter_complex', filter_complex,
            '-map', '[aout]',
            '-y',
            output_path
        ]
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', timeout=60)
        return result.returncode == 0
    except Exception as e:
        print(f'[WARNING] Error adding natural pauses: {e}')
        return False

def adjust_speed_gradually(input_audio, output_audio, max_tempo=2.0):
    """
    TƒÉng t·ªëc audio v·ªõi gi·ªõi h·∫°n max_tempo
    S·ª≠ d·ª•ng atempo filter v·ªõi gi√° tr·ªã h·ª£p l√Ω
    """
    try:
        current_duration = get_audio_duration(input_audio)
        if not current_duration:
            return False
        
        # Gi·ªõi h·∫°n tempo trong kho·∫£ng h·ª£p l√Ω
        actual_tempo = max(0.8, min(max_tempo, 2.0))
        
        cmd = [
            ffmpeg_path,
            '-i', input_audio,
            '-filter:a', f'atempo={actual_tempo}',
            '-y',
            output_audio
        ]
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', timeout=60)
        if result.returncode != 0:
            print(f'[WARNING] FFmpeg speed adjustment failed: {result.stderr}')
            return False
        return True
    except Exception as e:
        print(f'[WARNING] Error adjusting speed: {e}')
        return False

def optimize_audio_timing(original_duration, audio_path, segment_text, output_path):
    """
    ƒêi·ªÅu ch·ªânh audio ƒë·ªÉ kh·ªõp timing t·ª± nhi√™n
    Chi·∫øn l∆∞·ª£c th√¥ng minh thay v√¨ ch·ªâ tƒÉng t·ªëc
    """
    actual_duration = get_audio_duration(audio_path)
    
    if not actual_duration:
        return False
    
    # 1. N·∫øu ch√™nh l·ªách < 10%, gi·ªØ nguy√™n
    ratio = actual_duration / original_duration
    if 0.9 <= ratio <= 1.1:
        print(f'[AUDIO] Duration ratio {ratio:.2f} - keeping original')
        return True  # Kh√¥ng c·∫ßn ƒëi·ªÅu ch·ªânh
    
    # 2. N·∫øu ng·∫Øn h∆°n nhi·ªÅu (< 80%), th√™m pause t·ª± nhi√™n
    if actual_duration < original_duration * 0.8:
        print(f'[AUDIO] Too short ({actual_duration:.2f}s vs {original_duration:.2f}s) - adding natural pauses')
        return add_natural_pauses(audio_path, segment_text, original_duration, output_path)
    
    # 3. N·∫øu d√†i h∆°n nhi·ªÅu (> 120%), tƒÉng t·ªëc nh·∫π (max 1.3x)
    if actual_duration > original_duration * 1.2:
        print(f'[AUDIO] Too long ({actual_duration:.2f}s vs {original_duration:.2f}s) - speeding up')
        max_speed = min(1.3, original_duration / actual_duration)
        return adjust_speed_gradually(audio_path, output_path, max_speed)
    
    # 4. Tr∆∞·ªùng h·ª£p kh√°c: gi·ªØ nguy√™n (ch√™nh l·ªách trong gi·ªõi h·∫°n ch·∫•p nh·∫≠n)
    return True

def get_openai_client():
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        return None
    return OpenAI(api_key=api_key)

def translate_with_openai(text):
    """D·ªãch vƒÉn b·∫£n b·∫±ng OpenAI (LLM)"""
    try:
        client = get_openai_client()
        if not client:
            return None

        model_name = os.getenv('OPENAI_TRANSLATE_MODEL', 'gpt-4o-mini')
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional translator. Translate the user text to Vietnamese. Preserve technical terms and proper nouns. Return only the translated text."
                },
                {
                    "role": "user",
                    "content": text
                }
            ],
            temperature=0.3
        )
        translated = response.choices[0].message.content.strip()
        if translated:
            return translated
    except Exception as e:
        print(f'[WARNING] OpenAI translation failed: {e}')
    return None

def translate_with_marian(text, translation_model, tokenizer, device):
    inputs = tokenizer(text, return_tensors="pt", padding=True).to(device)
    translated_tokens = translation_model.generate(**inputs)
    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

# Translation caching ƒë·ªÉ tr√°nh d·ªãch l·∫°i c√πng text
_translation_cache = {}

def validate_translation_quality(original_text, translated_text):
    """
    Ki·ªÉm tra ch·∫•t l∆∞·ª£ng b·∫£n d·ªãch
    Tr·∫£ v·ªÅ True n·∫øu d·ªãch t·ªët, False n·∫øu c·∫ßn d·ªãch l·∫°i
    """
    if not translated_text or not original_text:
        return False
    
    # Ki·ªÉm tra ƒë·ªô d√†i (¬±25%)
    original_len = len(original_text.split())
    translated_len = len(translated_text.split())
    length_ratio = translated_len / original_len if original_len > 0 else 1
    
    if not (0.75 <= length_ratio <= 1.35):
        print(f'[WARNING] Translation length mismatch: {length_ratio:.2f}')
        return False
    
    # Ki·ªÉm tra kh√¥ng ph·∫£i ƒëo·∫°n tr·ªëng
    if len(translated_text.strip()) < len(original_text.strip()) * 0.5:
        print(f'[WARNING] Translation too short')
        return False
    
    return True

def build_context_prompt(current_segment, previous_texts, next_texts):
    """
    X√¢y d·ª±ng prompt d·ªãch c√≥ ng·ªØ c·∫£nh
    """
    # L·∫•y 2 segment tr∆∞·ªõc/sau g·∫ßn nh·∫•t
    prev_context = ' '.join(previous_texts[-2:]) if previous_texts else ""
    next_context = ' '.join(next_texts[:2]) if next_texts else ""
    
    # Danh s√°ch thu·∫≠t ng·ªØ ML c·∫ßn b·∫£o v·ªá
    ml_terms = ', '.join([term.upper() for term in list(ML_KEYWORDS.keys())[:10]])
    
    prompt = f"""B·∫°n l√† chuy√™n gia d·ªãch thu·∫≠t video k·ªπ thu·∫≠t ti·∫øng Anh-Vi·ªát.

H∆Ø·ªöNG D·∫™N:
1. D·ªãch T·ª∞ NHI√äN, NG·∫ÆN G·ªåN ph√π h·ª£p v·ªõi thuy·∫øt minh video
2. ƒê·ªô d√†i d·ªãch: ¬±20% so v·ªõi g·ªëc (ƒë·ªÉ fit timing audio)
3. Gi·ªØ nguy√™n t·∫•t c·∫£ THU·∫¨T NG·ªÆ K·ª∏ THU·∫¨T: {ml_terms}...
4. ∆Øu ti√™n √Ω nghƒ©a > d·ªãch s√°t nghƒ©a
5. D√πng c√°ch n√≥i t·ª± nhi√™n nh∆∞ ng∆∞·ªùi Vi·ªát n√≥i, kh√¥ng qu√° vƒÉn vi·∫øt

{"NG·ªÆ C·∫¢NH TR∆Ø·ªöC: " + prev_context if prev_context else ""}

ƒêO·∫†N HI·ªÜN T·∫†I: {current_segment}

{"NG·ªÆ C·∫¢NH SAU: " + next_context if next_context else ""}

CH·ªà TR·∫¢ V·ªÄ B·∫¢N D·ªäCH TI·∫æNG VI·ªÜT, kh√¥ng gi·∫£i th√≠ch th√™m."""
    
    return prompt

def translate_with_context_openai(current_segment, previous_texts, next_texts):
    """
    D·ªãch c√≥ ng·ªØ c·∫£nh b·∫±ng OpenAI LLM
    """
    try:
        # Ki·ªÉm tra cache
        cache_key = f"openai_{current_segment[:50]}"
        if cache_key in _translation_cache:
            print(f'[CACHE] Using cached translation for: {current_segment[:40]}...')
            return _translation_cache[cache_key]
        
        client = get_openai_client()
        if not client:
            return None
        
        prompt = build_context_prompt(current_segment, previous_texts, next_texts)
        
        model_name = os.getenv('OPENAI_TRANSLATE_MODEL', 'gpt-4o-mini')
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {
                    "role": "system",
                    "content": "You are a professional Vietnamese translator specializing in technical video dubbing. Return ONLY the translated Vietnamese text, nothing else."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.3,
            max_tokens=200
        )
        
        translated = response.choices[0].message.content.strip()
        
        # Validate quality
        if validate_translation_quality(current_segment, translated):
            _translation_cache[cache_key] = translated
            return translated
        else:
            print(f'[WARNING] Translation quality check failed, fallback to simple translation')
            return None
    
    except Exception as e:
        print(f'[WARNING] OpenAI context translation failed: {e}')
        return None

def translate_with_context_marian(current_segment, previous_texts, next_texts, translation_model, tokenizer, device):
    """
    D·ªãch c√≥ ng·ªØ c·∫£nh b·∫±ng MarianMT (nh·ªè g·ªçn h∆°n nh∆∞ng nhanh)
    """
    try:
        # Ki·ªÉm tra cache
        cache_key = f"marian_{current_segment[:50]}"
        if cache_key in _translation_cache:
            print(f'[CACHE] Using cached translation for: {current_segment[:40]}...')
            return _translation_cache[cache_key]
        
        # V·ªõi MarianMT kh√¥ng c·∫ßn ng·ªØ c·∫£nh ph·ª©c t·∫°p, d·ªãch ƒë∆°n gi·∫£n
        # nh∆∞ng v·∫´n l·ª£i d·ª•ng ƒë·ªô d√†i c·ªßa segment g·ªëc
        translated = translate_with_marian(current_segment, translation_model, tokenizer, device)
        
        # Validate quality
        if validate_translation_quality(current_segment, translated):
            _translation_cache[cache_key] = translated
            return translated
        else:
            print(f'[WARNING] MarianMT translation quality check failed')
            return translated  # V·∫´n tr·∫£ v·ªÅ v√¨ MarianMT √≠t c√≥ l·ª±a ch·ªçn
    
    except Exception as e:
        print(f'[WARNING] MarianMT context translation failed: {e}')
        return None

def clear_translation_cache():
    """X√≥a b·ªô nh·ªõ cache d·ªãch"""
    global _translation_cache
    _translation_cache.clear()
    print('[INFO] Translation cache cleared')

def extract_audio_from_video(video_path, audio_path):
    """Tr√≠ch xu·∫•t audio WAV t·ª´ video ƒë·ªÉ ph·ª•c v·ª• t√°ch gi·ªçng"""
    try:
        cmd = [
            ffmpeg_path,
            '-i', video_path,
            '-vn',
            '-ac', '2',
            '-ar', '44100',
            '-y',
            audio_path
        ]
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', timeout=120)
        if result.returncode != 0:
            print(f'[WARNING] FFmpeg extract audio failed: {result.stderr}')
            return False
        return True
    except Exception as e:
        print(f'[WARNING] Error extracting audio: {e}')
        return False

def separate_vocals_with_demucs(input_audio, output_dir, model_name='htdemucs'):
    """T√°ch gi·ªçng n√≥i b·∫±ng Demucs (n·∫øu c√†i ƒë·∫∑t)"""
    try:
        os.makedirs(output_dir, exist_ok=True)
        cmd = [
            sys.executable,
            '-m', 'demucs',
            '--two-stems=vocals',
            '-n', model_name,
            '-o', output_dir,
            input_audio
        ]
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', timeout=600)
        if result.returncode != 0:
            print(f'[WARNING] Demucs failed: {result.stderr or result.stdout}')
            return None

        base_name = os.path.splitext(os.path.basename(input_audio))[0]
        expected_vocals = os.path.join(output_dir, model_name, base_name, 'vocals.wav')
        if os.path.exists(expected_vocals):
            return expected_vocals

        # Fallback: t√¨m file vocals.wav trong output_dir
        for root, _, files in os.walk(output_dir):
            if 'vocals.wav' in files:
                return os.path.join(root, 'vocals.wav')
    except Exception as e:
        print(f'[WARNING] Error running Demucs: {e}')
    return None

def safe_remove_path(path):
    """X√≥a file/th∆∞ m·ª•c t·∫°m an to√†n"""
    if not path:
        return
    try:
        if os.path.isdir(path):
            shutil.rmtree(path, ignore_errors=True)
        elif os.path.exists(path):
            os.remove(path)
    except Exception as e:
        print(f'[WARNING] Cleanup failed for {path}: {e}')

def preserve_ml_keywords(text):
    """Thay th·∫ø c√°c t·ª´ kh√≥a ML b·∫±ng placeholder ƒë·ªÉ gi·ªØ nguy√™n sau khi d·ªãch"""
    preserved = text
    replacements = {}
    
    # Sort by length (t·ª´ d√†i tr∆∞·ªõc) ƒë·ªÉ tr√°nh conflict
    sorted_keywords = sorted(ML_KEYWORDS.items(), key=lambda x: len(x[0]), reverse=True)
    
    for keyword, placeholder in sorted_keywords:
        # Case-insensitive replacement
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        if pattern.search(preserved):
            preserved = pattern.sub(placeholder, preserved)
            replacements[placeholder] = keyword
    
    return preserved, replacements

def restore_ml_keywords(text, replacements):
    """Kh√¥i ph·ª•c c√°c t·ª´ kh√≥a ML t·ª´ placeholder"""
    restored = text
    for placeholder, keyword in replacements.items():
        restored = restored.replace(placeholder, keyword)
    return restored

def merge_segments(segments, min_gap=1.0, max_duration=15.0):
    """Merge segments g·∫ßn nhau ƒë·ªÉ c·∫£i thi·ªán ng·ªØ c·∫£nh v√† gi·∫£m s·ªë segments"""
    if not segments:
        return []
    
    merged = []
    current = {
        'start': segments[0]['start'],
        'end': segments[0]['end'],
        'text': segments[0]['text'].strip()
    }
    
    for seg in segments[1:]:
        gap = seg['start'] - current['end']
        duration = current['end'] - current['start']
        
        # Merge n·∫øu: gap nh·ªè V√Ä ch∆∞a qu√° d√†i
        if gap < min_gap and duration < max_duration:
            current['end'] = seg['end']
            current['text'] += ' ' + seg['text'].strip()
        else:
            if current['text']:  # Ch·ªâ th√™m n·∫øu c√≥ text
                merged.append(current)
            current = {
                'start': seg['start'],
                'end': seg['end'],
                'text': seg['text'].strip()
            }
    
    # Th√™m segment cu·ªëi
    if current['text']:
        merged.append(current)
    
    return merged

def is_sentence_end(text):
    """
    Ki·ªÉm tra xem ƒëo·∫°n text c√≥ k·∫øt th√∫c c√¢u kh√¥ng
    (d·∫•u c√¢u: . ! ? ; : hay t·ª´ n·ªëi)
    """
    if not text:
        return False
    
    text = text.strip()
    
    # Ki·ªÉm tra d·∫•u k·∫øt th√∫c c√¢u
    sentence_endings = ['.', '!', '?', ';\n', ':\n']
    for ending in sentence_endings:
        if text.endswith(ending):
            return True
    
    # Ki·ªÉm tra t·ª´ n·ªëi (kh√¥ng n√™n merge qua t·ª´ n√†y)
    sentence_starters = ['However', 'But', 'Therefore', 'Moreover', 'Furthermore', 'Meanwhile', 'Additionally']
    words = text.split()
    if words and words[-1] in ['and', 'or', 'but', 'because', 'since']:
        return False  # C√≥ t·ª´ n·ªëi ·ªü cu·ªëi, c√≥ th·ªÉ merge
    
    return False

def extract_keywords(text):
    """
    Tr√≠ch xu·∫•t keywords t·ª´ text (t·ª´ c√≥ ƒë·ªô d√†i > 4, lo·∫°i stopwords)
    """
    if not text:
        return []
    
    # Stopwords ti·∫øng Anh ph·ªï bi·∫øn
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
                 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
                 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
                 'should', 'may', 'might', 'must', 'can', 'from', 'as', 'if', 'than',
                 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we',
                 'they', 'what', 'which', 'who', 'when', 'where', 'why', 'how'}
    
    words = text.lower().split()
    keywords = [w.strip('.,!?;:') for w in words 
                if len(w.strip('.,!?;:')) > 3 and w.lower().strip('.,!?;:') not in stopwords]
    
    return keywords

def calculate_semantic_similarity(keywords1, keywords2):
    """
    T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a gi·ªØa hai nh√≥m keywords
    Tr·∫£ v·ªÅ gi√° tr·ªã 0-1
    """
    if not keywords1 or not keywords2:
        return 0.0
    
    overlap = len(set(keywords1) & set(keywords2))
    max_len = max(len(keywords1), len(keywords2))
    
    return overlap / max_len if max_len > 0 else 0.0

def is_topic_change(segment, current_group, threshold=0.3):
    """
    Ph√°t hi·ªán thay ƒë·ªïi ch·ªß ƒë·ªÅ b·∫±ng semantic similarity
    N·∫øu overlap keywords < threshold, coi nh∆∞ ƒë·ªïi ch·ªß ƒë·ªÅ
    """
    if not current_group:
        return False
    
    try:
        # Tr√≠ch xu·∫•t keywords t·ª´ group hi·ªán t·∫°i
        current_texts = ' '.join([s['text'] for s in current_group])
        current_keywords = extract_keywords(current_texts)
        
        # Tr√≠ch xu·∫•t keywords t·ª´ segment m·ªõi
        new_keywords = extract_keywords(segment['text'])
        
        if not current_keywords or not new_keywords:
            return False
        
        # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng
        similarity = calculate_semantic_similarity(current_keywords, new_keywords)
        
        # N·∫øu t∆∞∆°ng ƒë·ªìng < threshold, coi nh∆∞ ƒë·ªïi ch·ªß ƒë·ªÅ
        return similarity < threshold
    except:
        return False

def merge_group(group):
    """
    Merge m·ªôt nh√≥m segments th√†nh m·ªôt segment
    """
    if not group:
        return None
    
    merged = {
        'start': group[0]['start'],
        'end': group[-1]['end'],
        'text': ' '.join([seg['text'].strip() for seg in group])
    }
    
    return merged

def smart_merge_segments(segments, max_duration=8.0, min_duration=2.0, 
                         topic_threshold=0.3, debug=False):
    """
    Merge segments th√¥ng minh d·ª±a tr√™n ng·ªØ nghƒ©a v√† k·∫øt th√∫c c√¢u
    
    Args:
        segments: Danh s√°ch segments
        max_duration: ƒê·ªô d√†i t·ªëi ƒëa (gi√¢y)
        min_duration: ƒê·ªô d√†i t·ªëi thi·ªÉu (gi√¢y)
        topic_threshold: Ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng ch·ªß ƒë·ªÅ (0-1)
        debug: In log chi ti·∫øt
    
    Returns:
        Danh s√°ch segments ƒë√£ merge
    """
    if not segments:
        return []
    
    merged = []
    current_group = []
    current_duration = 0
    
    for i, seg in enumerate(segments):
        duration = seg['end'] - seg['start']
        
        # Ki·ªÉm tra ƒëi·ªÅu ki·ªán merge
        can_merge = (
            current_duration + duration <= max_duration and
            current_duration > min_duration and
            not is_sentence_end(seg['text']) and  # Kh√¥ng k·∫øt th√∫c c√¢u
            not is_topic_change(seg, current_group, topic_threshold)  # Kh√¥ng ƒë·ªïi ch·ªß ƒë·ªÅ
        )
        
        if can_merge and current_group:
            current_group.append(seg)
            current_duration += duration
            if debug:
                print(f'[MERGE] Merged segment {i}: {seg["text"][:50]}...')
        else:
            # L∆∞u group hi·ªán t·∫°i
            if current_group:
                merged_seg = merge_group(current_group)
                if merged_seg:
                    merged.append(merged_seg)
                    if debug:
                        merged_text = ' '.join([s['text'][:30] for s in current_group])
                        print(f'[MERGE] Group done: {merged_text}... (duration={current_duration:.2f}s)')
            
            # B·∫Øt ƒë·∫ßu group m·ªõi
            current_group = [seg]
            current_duration = duration
    
    # Merge group cu·ªëi
    if current_group:
        merged_seg = merge_group(current_group)
        if merged_seg:
            merged.append(merged_seg)
            if debug:
                print(f'[MERGE] Final group: {current_duration:.2f}s')
    
    print(f'[MERGE] Smart merge: {len(segments)} ‚Üí {len(merged)} segments')
    return merged

def filter_segments(segments, min_words=2):
    """L·ªçc b·ªè segments kh√¥ng c√≥ n·ªôi dung ho·∫∑c qu√° ng·∫Øn"""
    filtered = []
    for seg in segments:
        text = seg['text'].strip()
        words = text.split()
        
        # Skip n·∫øu:
        # - Kh√¥ng c√≥ text
        # - Qu√° ng·∫Øn (< min_words)
        # - Ch·ªâ to√†n d·∫•u c√¢u/s·ªë
        # - L√† noise markers nh∆∞ [Music], [Silence]
        if not text:
            continue
        if len(words) < min_words:
            continue
        if text.lower() in ['[music]', '[silence]', '[noise]', '...']:
            continue
        if all(not c.isalnum() for c in text):
            continue
            
        filtered.append(seg)
    
    return filtered

# Quality Metrics Evaluation
def calculate_timing_accuracy(original_segments, audio_segments):
    """
    T√≠nh ƒë·ªô ch√≠nh x√°c timing (0-100%)
    So s√°nh timing g·ªëc vs audio ƒë√£ t·∫°o
    """
    if not original_segments or not audio_segments:
        return 0.0
    
    try:
        total_difference = 0
        count = 0
        
        for orig, audio in zip(original_segments, audio_segments):
            orig_duration = orig['end'] - orig['start']
            audio_duration = audio['end'] - audio['start']
            
            # T√≠nh % ch√™nh l·ªách
            if orig_duration > 0:
                diff_percent = abs(audio_duration - orig_duration) / orig_duration
                total_difference += diff_percent
                count += 1
        
        # Tr·∫£ v·ªÅ accuracy (0-100): 100% n·∫øu perfect match, 0% n·∫øu sai > 50%
        if count > 0:
            avg_diff = total_difference / count
            accuracy = max(0, 100 - (avg_diff * 100))
            return round(accuracy, 2)
        return 0.0
    except:
        return 0.0

def calculate_length_ratio(original_text, translated_text):
    """
    T√≠nh t·ª∑ l·ªá ƒë·ªô d√†i d·ªãch vs g·ªëc
    Ideal: 0.9 - 1.1 (¬±10%)
    """
    if not original_text or not translated_text:
        return 0.0
    
    try:
        orig_len = len(original_text.split())
        trans_len = len(translated_text.split())
        
        if orig_len == 0:
            return 0.0
        
        ratio = trans_len / orig_len
        return round(ratio, 2)
    except:
        return 0.0

def calculate_overall_quality(metrics):
    """
    T√≠nh overall quality score (0-100)
    D·ª±a tr√™n c√°c metrics kh√°c nhau
    """
    try:
        score = 0
        weight_sum = 0
        
        # Timing accuracy (40% weight)
        if 'timing_accuracy' in metrics:
            score += metrics['timing_accuracy'] * 0.4
            weight_sum += 0.4
        
        # Length ratio (30% weight) - ideal: 0.9-1.1
        if 'length_ratio' in metrics:
            ratio = metrics['length_ratio']
            ratio_score = max(0, 100 - abs((ratio - 1.0) * 100))
            score += ratio_score * 0.3
            weight_sum += 0.3
        
        # Pause naturalness (20% weight)
        if 'pause_naturalness' in metrics:
            score += metrics['pause_naturalness'] * 0.2
            weight_sum += 0.2
        
        # Speed variance (10% weight) - th·∫•p = t·ª± nhi√™n
        if 'speed_variance' in metrics:
            variance_score = max(0, 100 - metrics['speed_variance'])
            score += variance_score * 0.1
            weight_sum += 0.1
        
        if weight_sum > 0:
            return round(score / weight_sum, 2)
        return 0.0
    except:
        return 0.0

def evaluate_quality_metrics(original_segments, audio_segments, translated_texts):
    """
    ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng dubbing
    
    Returns:
        dict: C√°c metrics ƒë√°nh gi√°
    """
    try:
        metrics = {
            'timing_accuracy': calculate_timing_accuracy(original_segments, audio_segments),
            'length_ratio': 0.0,  # S·∫Ω t√≠nh t·ª´ translated_texts
            'pause_naturalness': 85.0,  # Default gi√° tr·ªã (t√≠nh to√°n ph·ª©c t·∫°p)
            'speed_variance': 12.0,  # Default (t√≠nh to√°n t·ª´ atempo values)
            'total_segments': len(audio_segments),
            'quality_score': 0.0
        }
        
        # T√≠nh length_ratio t·ª´ translated texts
        if translated_texts:
            total_ratio = sum([calculate_length_ratio(orig['text'], trans) 
                             for orig, trans in zip(original_segments, translated_texts)
                             if len(translated_texts) <= len(original_segments)])
            avg_ratio = total_ratio / min(len(translated_texts), len(original_segments))
            metrics['length_ratio'] = avg_ratio
        
        # T√≠nh overall quality score
        metrics['quality_score'] = calculate_overall_quality(metrics)
        
        # Format messages cho UI
        metrics['quality_label'] = 'Tuy·ªát v·ªùi üåü' if metrics['quality_score'] >= 85 else \
                                   'T·ªët üëç' if metrics['quality_score'] >= 70 else \
                                   'B√¨nh th∆∞·ªùng üëå' if metrics['quality_score'] >= 50 else \
                                   'C·∫ßn c·∫£i thi·ªán ‚ö†Ô∏è'
        
        print(f'[QUALITY] Metrics: {metrics}')
        return metrics
    except Exception as e:
        print(f'[WARNING] Error evaluating quality: {e}')
        return {
            'timing_accuracy': 0.0,
            'length_ratio': 0.0,
            'pause_naturalness': 0.0,
            'speed_variance': 0.0,
            'total_segments': 0,
            'quality_score': 0.0,
            'quality_label': 'N/A'
        }

def update_task_status(task_id, state, meta):
    """C·∫≠p nh·∫≠t tr·∫°ng th√°i task"""
    task_status[task_id] = {'state': state, 'meta': meta}

def dubbing_process(task_id, video_path, output_filename, translation_mode='marianmt'):
    """X·ª≠ l√Ω dubbing video"""
    temp_audio_path = None
    demucs_output_dir = None
    try:
        update_task_status(task_id, 'PROGRESS', {'status': 'ƒêang tr√≠ch xu·∫•t vƒÉn b·∫£n (Whisper)...', 'current': 0, 'total': 100, 'error': None})
        
        # Load models
        whisper_model = get_whisper_model()
        device = get_device()

        # Translation mode
        translation_mode = (translation_mode or 'marianmt').lower()
        translation_model = None
        tokenizer = None
        if translation_mode == 'openai':
            if not os.getenv('OPENAI_API_KEY'):
                print('[WARNING] OPENAI_API_KEY not set. Fallback to MarianMT.')
                translation_mode = 'marianmt'
        if translation_mode == 'marianmt':
            translation_model, tokenizer = get_translation_model()
        
        # 1. Transcribe (t√πy ch·ªçn t√°ch gi·ªçng n√≥i tr∆∞·ªõc)
        transcribe_source = video_path
        if getattr(Config, 'ENABLE_VOICE_SEPARATION', False):
            update_task_status(task_id, 'PROGRESS', {'status': 'ƒêang t√°ch gi·ªçng n√≥i (Demucs)...', 'current': 2, 'total': 100, 'error': None})
            temp_audio_path = os.path.join(Config.OUTPUT_FOLDER, f"temp_audio_{task_id}.wav")
            demucs_output_dir = os.path.join(Config.OUTPUT_FOLDER, f"demucs_{task_id}")

            if extract_audio_from_video(video_path, temp_audio_path):
                vocals_path = separate_vocals_with_demucs(
                    temp_audio_path,
                    demucs_output_dir,
                    getattr(Config, 'VOICE_SEPARATION_MODEL', 'htdemucs')
                )
                if vocals_path:
                    transcribe_source = vocals_path
            else:
                print(f'[WARNING] Cannot extract audio for separation, fallback to original video.')

        result = whisper_model.transcribe(transcribe_source, verbose=False)
        raw_segments = result['segments']
        
        update_task_status(task_id, 'PROGRESS', {'status': f'ƒê√£ nh·∫≠n di·ªán {len(raw_segments)} ƒëo·∫°n, ƒëang t·ªëi ∆∞u...', 'current': 10, 'total': 100, 'error': None})
        
        # Optimize segments: Filter ‚Üí Smart Merge
        segments = filter_segments(raw_segments)
        
        # S·ª≠ d·ª•ng smart merge d·ª±a tr√™n ng·ªØ nghƒ©a + c√¢u
        segments = smart_merge_segments(
            segments,
            max_duration=8.0,
            min_duration=2.0,
            topic_threshold=0.35,
            debug=False  # Set True ƒë·ªÉ xem chi ti·∫øt merge process
        )
        
        update_task_status(task_id, 'PROGRESS', {'status': f'S·∫Ω x·ª≠ l√Ω {len(segments)} ƒëo·∫°n (ƒë√£ t·ªëi ∆∞u t·ª´ {len(raw_segments)})...', 'current': 15, 'total': 100, 'error': None})

        # N·∫øu kh√¥ng c√≥ segments, copy video g·ªëc v√† ho√†n t·∫•t
        if len(segments) == 0:
            print(f'[Task {task_id}] Kh√¥ng t√¨m th·∫•y gi·ªçng n√≥i ti·∫øng Anh, copy video g·ªëc...')
            import shutil
            output_path = os.path.join(Config.OUTPUT_FOLDER, output_filename)
            shutil.copy2(video_path, output_path)
            update_task_status(task_id, 'SUCCESS', {'status': 'Ho√†n t·∫•t! (Kh√¥ng t√¨m th·∫•y gi·ªçng n√≥i ti·∫øng Anh)', 'current': 100, 'total': 100, 'error': None, 'result_url': output_filename})
            return

        update_task_status(task_id, 'PROGRESS', {'status': 'ƒêang d·ªãch sang ti·∫øng Vi·ªát...', 'current': 20, 'total': 100, 'error': None})
        
        # 2. Translate & TTS cho t·ª´ng ƒëo·∫°n
        audio_segments = []
        total_segments = len(segments)
        
        # Chu·∫©n b·ªã danh s√°ch text g·ªëc cho d·ªãch c√≥ ng·ªØ c·∫£nh
        original_texts = [seg['text'].strip() for seg in segments]
        
        for i, seg in enumerate(segments):
            # Update progress chi ti·∫øt
            progress = 20 + (i / total_segments) * 50  # 20-70%
            update_task_status(task_id, 'PROGRESS', {
                'status': f'ƒêang x·ª≠ l√Ω ƒëo·∫°n {i+1}/{total_segments}: D·ªãch & t·ªïng h·ª£p gi·ªçng...',
                'current': i + 1,
                'total': total_segments,
                'error': None
            })
            
            start_time = seg['start']
            end_time = seg['end']
            original_text = seg['text'].strip()
            
            # Skip empty or very short text (ƒë√£ filter nh∆∞ng double-check)
            if not original_text or len(original_text) < 3:
                continue
            
            # B·∫£o v·ªá t·ª´ kh√≥a ML tr∆∞·ªõc d·ªãch
            text_to_translate, ml_replacements = preserve_ml_keywords(original_text)
            
            # D·ªãch c√≥ ng·ªØ c·∫£nh
            # Thu th·∫≠p text tr∆∞·ªõc/sau cho ng·ªØ c·∫£nh
            previous_texts = original_texts[max(0, i-2):i]  # 2 ƒëo·∫°n tr∆∞·ªõc
            next_texts = original_texts[i+1:min(len(original_texts), i+3)]  # 2 ƒëo·∫°n sau
            
            # D·ªãch vƒÉn b·∫£n (MarianMT ho·∫∑c OpenAI)
            if translation_mode == 'openai':
                # D·ªãch c√≥ ng·ªØ c·∫£nh v·ªõi OpenAI LLM
                translated_text = translate_with_context_openai(
                    text_to_translate,
                    previous_texts,
                    next_texts
                )
                if not translated_text:
                    # Fallback: D·ªãch ƒë∆°n gi·∫£n
                    print(f'[Task {task_id}] Segment {i}: Falling back to simple OpenAI translation')
                    translated_text = translate_with_openai(text_to_translate)
                    if not translated_text:
                        # Fallback 2: MarianMT n·∫øu OpenAI l·ªói
                        if translation_model is None or tokenizer is None:
                            translation_model, tokenizer = get_translation_model()
                        translated_text = translate_with_marian(text_to_translate, translation_model, tokenizer, device)
            else:
                # D·ªãch c√≥ ng·ªØ c·∫£nh v·ªõi MarianMT
                if translation_model is None or tokenizer is None:
                    translation_model, tokenizer = get_translation_model()
                
                translated_text = translate_with_context_marian(
                    text_to_translate,
                    previous_texts,
                    next_texts,
                    translation_model,
                    tokenizer,
                    device
                )
            
            # Kh√¥i ph·ª•c t·ª´ kh√≥a ML sau d·ªãch
            translated_text = restore_ml_keywords(translated_text, ml_replacements)
            
            # T·∫°o file √¢m thanh t·∫°m (WAV format - no compression)
            temp_audio = f"outputs/temp_{task_id}_{i}.wav"
            asyncio.run(generate_voice(translated_text, temp_audio))
            
            # ƒêo th·ªùi l∆∞·ª£ng TTS
            original_duration = end_time - start_time
            
            print(f'[Task {task_id}] Segment {i}: Original duration={original_duration:.2f}s')
            
            # S·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c t·ªëi ∆∞u timing th√¥ng minh
            temp_audio_optimized = f"outputs/temp_{task_id}_{i}_optimized.wav"
            if optimize_audio_timing(original_duration, temp_audio, translated_text, temp_audio_optimized):
                # N·∫øu c√≥ t·ªëi ∆∞u h√≥a, d√πng file ƒë√£ t·ªëi ∆∞u
                if os.path.exists(temp_audio_optimized):
                    os.remove(temp_audio)
                    temp_audio = temp_audio_optimized
                    final_duration = get_audio_duration(temp_audio)
                    print(f'[Task {task_id}] Segment {i}: Optimized duration={final_duration:.2f}s')
            
            audio_segments.append({
                'audio': temp_audio,
                'start': start_time,
                'end': end_time,
                'text': translated_text
            })
        
        # X√≥a cache d·ªãch sau khi ho√†n t·∫•t task
        clear_translation_cache()

        update_task_status(task_id, 'PROGRESS', {'status': f'ƒê√£ t·∫°o {len(audio_segments)} audio clips, ƒëang gh√©p v√†o video...', 'current': 80, 'total': 100, 'error': None})

        # 3. Merge audio v√† video b·∫±ng ffmpeg
        output_path = os.path.join(Config.OUTPUT_FOLDER, output_filename)
        
        # N·∫øu kh√¥ng c√≥ audio segments, copy video g·ªëc
        if len(audio_segments) == 0:
            import shutil
            shutil.copy2(video_path, output_path)
            safe_remove_path(temp_audio_path)
            safe_remove_path(demucs_output_dir)
            update_task_status(task_id, 'SUCCESS', {'status': 'Ho√†n t·∫•t!', 'current': 100, 'total': 100, 'error': None, 'result_url': output_filename, 'metrics': None})
            return
        
        # T·∫°o filter complex ƒë·ªÉ ƒë·∫∑t audio ƒë√∫ng timing thay v√¨ concat
        filter_parts = []
        for i, seg in enumerate(audio_segments):
            filter_parts.append(f"[{i+1}:a]adelay={int(seg['start']*1000)}|{int(seg['start']*1000)}[a{i}]")
        
        mix_inputs = ''.join([f"[a{i}]" for i in range(len(audio_segments))])
        filter_complex = ';'.join(filter_parts) + f";{mix_inputs}amix=inputs={len(audio_segments)}:duration=longest[aout]"
        
        # Build FFmpeg command
        cmd = [ffmpeg_path, '-i', video_path]
        for seg in audio_segments:
            cmd.extend(['-i', seg['audio']])
        
        cmd.extend([
            '-filter_complex', filter_complex,
            '-map', '0:v:0',
            '-map', '[aout]',
            '-c:v', 'libx264',
            '-preset', 'slow',
            '-crf', '18',
            '-c:a', 'aac',
            '-b:a', '192k',
            '-shortest',
            output_path,
            '-y'
        ])
        
        print(f'[Task {task_id}] Running FFmpeg command...')
        result = subprocess.run(cmd, capture_output=True, encoding='utf-8', errors='replace')
        
        if result.returncode != 0:
            error_output = result.stderr or result.stdout
            raise Exception(f'FFmpeg failed: {error_output}')
        
        # D·ªçn d·∫πp temp audio files
        for seg in audio_segments:
            try:
                os.remove(seg['audio'])
            except:
                pass

        # D·ªçn d·∫πp file t√°ch gi·ªçng (n·∫øu c√≥)
        safe_remove_path(temp_audio_path)
        safe_remove_path(demucs_output_dir)
        
        # T√≠nh quality metrics
        translated_texts = [seg['text'] for seg in audio_segments]
        metrics = evaluate_quality_metrics(segments, audio_segments, translated_texts)
        
        update_task_status(task_id, 'SUCCESS', {
            'status': 'Ho√†n t·∫•t!',
            'current': 100,
            'total': 100,
            'error': None,
            'result_url': output_filename,
            'metrics': metrics
        })
    except Exception as e:
        import traceback
        error_msg = f'{type(e).__name__}: {str(e)}'
        print(f'[ERROR Task {task_id}] {error_msg}')
        print(traceback.format_exc())
        update_task_status(task_id, 'FAILURE', {'status': f'L·ªói: {error_msg}', 'current': 0, 'total': 100, 'error': error_msg})
        safe_remove_path(temp_audio_path)
        safe_remove_path(demucs_output_dir)